{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPi/uEOinctWcM1o4ghQbGG"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "NJ-DqwdfMbJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87XAV2puMFZb"
      },
      "outputs": [],
      "source": [
        "class IteratedPrisonersDilemma:\n",
        "    def __init__(self, max_steps):\n",
        "        \"\"\"\n",
        "        max_steps: number of steps (rounds) per episode\n",
        "        \"\"\"\n",
        "        self.max_steps = max_steps\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        # History of (actionA, actionB). Each action is 0=C, 1=D\n",
        "        self.history = []\n",
        "        self.t = 0\n",
        "        return self.history\n",
        "\n",
        "    def step(self, actionA, actionB):\n",
        "        \"\"\"\n",
        "        actionA, actionB in {0, 1} where:\n",
        "          0 = Cooperate, 1 = Defect\n",
        "        \"\"\"\n",
        "        self.history.append((actionA, actionB))\n",
        "\n",
        "        # Calculate reward\n",
        "        if actionA == 0 and actionB == 0:\n",
        "            # Both cooperate\n",
        "            rewardA, rewardB = 3, 3\n",
        "        elif actionA == 1 and actionB == 1:\n",
        "            # Both defect\n",
        "            rewardA, rewardB = 1, 1\n",
        "        elif actionA == 0 and actionB == 1:\n",
        "            # A cooperates, B defects\n",
        "            rewardA, rewardB = 0, 5\n",
        "        else:\n",
        "            # A defects, B cooperates\n",
        "            rewardA, rewardB = 5, 0\n",
        "\n",
        "        self.t += 1\n",
        "        done = (self.t >= self.max_steps)\n",
        "        return self.history, rewardA, rewardB, done"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple mapping for pairs -> int\n",
        "pair_to_int = {\n",
        "    (0, 0): 0,\n",
        "    (0, 1): 1,\n",
        "    (1, 0): 2,\n",
        "    (1, 1): 3\n",
        "}"
      ],
      "metadata": {
        "id": "9RNldtQfMU8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "    \"\"\"\n",
        "    Generate a causal mask for a sequence of length `sz`.\n",
        "    \"\"\"\n",
        "    # Upper triangular matrix of 1s, shifted by 1 so the diagonal is all allowed\n",
        "    # positions. This means position i can attend up to i (including itself),\n",
        "    # but not beyond.\n",
        "    mask = torch.triu(torch.ones(sz, sz), diagonal=1)\n",
        "    # Convert 1 -> -inf and 0 -> 0. This tells attention to ignore future tokens.\n",
        "    mask = mask.masked_fill(mask == 1, float('-inf'))\n",
        "    return mask"
      ],
      "metadata": {
        "id": "1W8JMKDQONiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerPolicy(nn.Module):\n",
        "    def __init__(self,\n",
        "                 max_length,\n",
        "                 d_model=32,\n",
        "                 nhead=4,\n",
        "                 num_layers=2,\n",
        "                 device='cpu'):\n",
        "        super().__init__()\n",
        "        self.max_length = max_length\n",
        "        self.device = device\n",
        "\n",
        "        # We have 4 possible tokens for the pair-history\n",
        "        self.vocab_size = 4\n",
        "        self.embedding = nn.Embedding(self.vocab_size, d_model)\n",
        "\n",
        "        # Positional encoding\n",
        "        self.pos_encoding = nn.Embedding(self.max_length, d_model)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model,\n",
        "                                                   nhead=nhead,\n",
        "                                                   dim_feedforward=64)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer,\n",
        "                                                         num_layers=num_layers)\n",
        "\n",
        "        # Output layer: 2 actions (Cooperate or Defect)\n",
        "        self.action_head = nn.Linear(d_model, 2)\n",
        "\n",
        "    def forward(self, history_tokens):\n",
        "        \"\"\"\n",
        "        history_tokens: [batch_size, seq_len] (integers in [0..3])\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = history_tokens.shape\n",
        "\n",
        "        # Token embedding\n",
        "        x = self.embedding(history_tokens)  # shape: [batch_size, seq_len, d_model]\n",
        "\n",
        "        # Add positional encodings\n",
        "        positions = torch.arange(seq_len, device=self.device).unsqueeze(0)  # [1, seq_len]\n",
        "        pos_emb = self.pos_encoding(positions)  # [1, seq_len, d_model]\n",
        "        x = x + pos_emb\n",
        "\n",
        "        # The Transformer in PyTorch expects [seq_len, batch_size, d_model]\n",
        "        x = x.permute(1, 0, 2)  # -> [seq_len, batch_size, d_model]\n",
        "\n",
        "        causal_mask = generate_square_subsequent_mask(seq_len).to(self.device)\n",
        "\n",
        "        # Forward through the Transformer\n",
        "        encoded = self.transformer_encoder(x, mask=causal_mask)  # [seq_len, batch_size, d_model]\n",
        "\n",
        "        # Take the last hidden state (the final tokenâ€™s representation)\n",
        "        last_hidden = encoded[-1, :, :]  # shape: [batch_size, d_model]\n",
        "\n",
        "        # Output logits for next action\n",
        "        logits = self.action_head(last_hidden)  # [batch_size, 2]\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "7WgwIQPWMkF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Player B strategies\n",
        "\n",
        "import random\n",
        "\n",
        "def always_cooperate(historyA, historyB):\n",
        "    \"\"\"\n",
        "    Always Cooperate Strategy:\n",
        "    Player B always cooperates, regardless of history.\n",
        "    \"\"\"\n",
        "    return 0  # Always cooperate\n",
        "\n",
        "\n",
        "def always_defect(historyA, historyB):\n",
        "    \"\"\"\n",
        "    Always Defect Strategy:\n",
        "    Player B always defects, regardless of history.\n",
        "    \"\"\"\n",
        "    return 1  # Always defect\n",
        "\n",
        "\n",
        "def tit_for_tat(historyA, historyB):\n",
        "    \"\"\"\n",
        "    Tit-for-Tat Strategy:\n",
        "    - On the first move, cooperate.\n",
        "    - On subsequent moves, do whatever Player A did in the previous round.\n",
        "    \"\"\"\n",
        "    if not historyB:\n",
        "        return 0  # First move: cooperate\n",
        "    else:\n",
        "        return historyA[-1]  # Mimic the opponent's last move\n",
        "\n",
        "\n",
        "def suspicious_tit_for_tat(historyA, historyB):\n",
        "    \"\"\"\n",
        "    Suspicious Tit-for-Tat:\n",
        "    - On the first move, defect (be suspicious).\n",
        "    - On subsequent moves, do whatever Player A did in the previous round.\n",
        "    \"\"\"\n",
        "    if not historyB:\n",
        "        return 1  # First move: defect\n",
        "    else:\n",
        "        return historyA[-1]\n",
        "\n",
        "\n",
        "def grudger(historyA, historyB):\n",
        "    \"\"\"\n",
        "    Grudger (a.k.a. Grim Trigger):\n",
        "    - Cooperate unless the opponent has ever defected in the past.\n",
        "    - If the opponent defects once, defect forever.\n",
        "    \"\"\"\n",
        "    if 1 in historyA:\n",
        "        # If the opponent has ever defected, defect forever\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "def pavlov(historyA, historyB):\n",
        "    \"\"\"\n",
        "    Pavlov (a.k.a. Win-Stay, Lose-Shift):\n",
        "    - If both players made the same move in the previous round (both cooperated or both defected), cooperate this round.\n",
        "    - Otherwise (if one cooperated and the other defected), defect this round.\n",
        "    - First move: cooperate by default.\n",
        "    \"\"\"\n",
        "    if not historyB:\n",
        "        return 0  # No history yet, default to cooperate\n",
        "    else:\n",
        "        last_move_A = historyA[-1]\n",
        "        last_move_B = historyB[-1]\n",
        "        if last_move_A == last_move_B:\n",
        "            # If last round was CC or DD, cooperate\n",
        "            return 0\n",
        "        else:\n",
        "            # If last round was CD or DC, defect\n",
        "            return 1\n",
        "\n",
        "\n",
        "def random_strategy(historyA, historyB):\n",
        "    \"\"\"\n",
        "    Random Strategy:\n",
        "    - Player B chooses randomly between cooperate (0) and defect (1) each round.\n",
        "    \"\"\"\n",
        "    return random.randint(0, 1)\n",
        "\n",
        "\n",
        "def bully(historyA, historyB):\n",
        "    \"\"\"\n",
        "    Bully Strategy:\n",
        "    - First round: Defect.\n",
        "    - If the opponent cooperated last round, continue defecting.\n",
        "    - If the opponent defected last round, switch to cooperating.\n",
        "    \"\"\"\n",
        "    if not historyB:\n",
        "        return 1  # First move: defect\n",
        "    else:\n",
        "        if historyA[-1] == 0:\n",
        "            # Opponent cooperated last time -> keep defecting\n",
        "            return 1\n",
        "        else:\n",
        "            # Opponent defected last time -> try cooperating\n",
        "            return 0\n",
        "\n",
        "\n",
        "def random_tft(historyA, historyB):\n",
        "    \"\"\"\n",
        "    Randomized Tit-for-Tat:\n",
        "    - Like Tit-for-Tat, but occasionally (with small probability) defect anyway,\n",
        "      to inject some unpredictability.\n",
        "    \"\"\"\n",
        "    if not historyB:\n",
        "        return 0  # First move: cooperate\n",
        "\n",
        "    # Typically, do what the opponent did last time\n",
        "    move = historyA[-1]\n",
        "\n",
        "    # With a small probability (say 10%), do the opposite for randomness\n",
        "    if random.random() < 0.1:\n",
        "        move = 1 - move\n",
        "    return move\n",
        "\n",
        "\n",
        "def detective(historyA, historyB):\n",
        "    \"\"\"\n",
        "    Detective Strategy (a playful example):\n",
        "    - Start with a sequence of moves to 'test' the opponent: Cooperate, Defect, Cooperate, Cooperate.\n",
        "    - After the initial testing phase, follow a Tit-for-Tat approach,\n",
        "      but if ever the opponent defects in response to your testing, grudges can form.\n",
        "    \"\"\"\n",
        "    # Predefined opening sequence for the first 4 moves:\n",
        "    opening_sequence = [0, 1, 0, 0]  # C, D, C, C\n",
        "\n",
        "    # If still in the opening phase, follow the script:\n",
        "    if len(historyB) < 4:\n",
        "        return opening_sequence[len(historyB)]\n",
        "\n",
        "    # Post-opening: revert to a more typical approach (here, Tit-for-Tat)\n",
        "    return tit_for_tat(historyA, historyB)\n",
        "\n",
        "\n",
        "def tit_for_two_tats(historyA, historyB):\n",
        "    \"\"\"\n",
        "    Tit-for-Two-Tats:\n",
        "    - Cooperate by default until the opponent defects twice in a row.\n",
        "    - Once you see two consecutive defects, defect next round.\n",
        "    - Then go back to cooperating, but keep watching.\n",
        "    \"\"\"\n",
        "    # If there's less than 2 rounds of history, cooperate\n",
        "    if len(historyB) < 2:\n",
        "        return 0\n",
        "\n",
        "    # If opponent defected in the last two rounds, defect\n",
        "    if historyA[-1] == 1 and historyA[-2] == 1:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "def generous_tit_for_tat(historyA, historyB):\n",
        "    \"\"\"\n",
        "    Generous Tit-for-Tat:\n",
        "    - Normally, copy the opponent's last move (like TFT).\n",
        "    - But if the opponent defected last round, cooperate anyway with a certain probability\n",
        "      (e.g., 30% chance to forgive).\n",
        "    \"\"\"\n",
        "    forgiveness_probability = 0.3\n",
        "\n",
        "    if not historyB:\n",
        "        return 0  # first move: cooperate\n",
        "\n",
        "    if historyA[-1] == 1:\n",
        "        # Opponent defected\n",
        "        if random.random() < forgiveness_probability:\n",
        "            return 0  # Forgive and cooperate\n",
        "        else:\n",
        "            return 1  # Defect as retaliation\n",
        "    else:\n",
        "        # Opponent cooperated last round\n",
        "        return 0\n",
        "\n",
        "\n",
        "def adaptive_strategy(historyA, historyB):\n",
        "    \"\"\"\n",
        "    Adaptive Strategy:\n",
        "    - Keep track of the opponent's cooperation ratio in the past few rounds (e.g., last 5).\n",
        "    - If the ratio of opponent's cooperation is above a threshold, cooperate; otherwise defect.\n",
        "    \"\"\"\n",
        "    window_size = 5\n",
        "    threshold = 0.5\n",
        "\n",
        "    if not historyB:\n",
        "        return 0  # first move: cooperate\n",
        "\n",
        "    # Look at opponent's last `window_size` moves\n",
        "    recent_moves = historyA[-window_size:]\n",
        "    cooperation_ratio = sum(1 for move in recent_moves if move == 0) / len(recent_moves)\n",
        "\n",
        "    if cooperation_ratio > threshold:\n",
        "        return 0  # cooperate\n",
        "    else:\n",
        "        return 1  # defect\n",
        "\n",
        "\n",
        "def lookback_defect_if_opponent_defected_x_rounds_ago(historyA, historyB, x=2):\n",
        "    \"\"\"\n",
        "    Lookback Strategy (Parameterizable):\n",
        "    - Cooperate unless the opponent defected exactly x rounds ago.\n",
        "    - For example, if x=2, always 'retaliate' 2 rounds after the opponent defects.\n",
        "    - This function is written in a more general form.\n",
        "    \"\"\"\n",
        "    if len(historyA) < x:\n",
        "        return 0  # not enough history, cooperate\n",
        "    if historyA[-x] == 1:\n",
        "        return 1  # defect if the opponent defected x rounds ago\n",
        "    return 0\n",
        "\n",
        "manual_strategies = [\n",
        "    always_cooperate,\n",
        "    always_defect,\n",
        "    tit_for_tat,\n",
        "    suspicious_tit_for_tat,\n",
        "    grudger,\n",
        "    pavlov,\n",
        "    random_strategy,\n",
        "    bully,\n",
        "    random_tft,\n",
        "    detective,\n",
        "    tit_for_two_tats,\n",
        "    generous_tit_for_tat,\n",
        "    adaptive_strategy,\n",
        "    lookback_defect_if_opponent_defected_x_rounds_ago\n",
        "]\n"
      ],
      "metadata": {
        "id": "_CmhBQaOo_QO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def snapshot_policy(policy):\n",
        "    # returns a new policy with the same weights\n",
        "    new_policy = TransformerPolicy(\n",
        "        max_length=policy.max_length,\n",
        "        d_model=policy.embedding.embedding_dim,\n",
        "        nhead=policy.transformer_encoder.layers[0].self_attn.num_heads,\n",
        "        num_layers=len(policy.transformer_encoder.layers),\n",
        "        device=policy.device,\n",
        "    )\n",
        "    new_policy.load_state_dict(policy.state_dict())\n",
        "    new_policy.to(new_policy.device)\n",
        "    new_policy.eval()  # freeze it\n",
        "    return new_policy\n",
        "\n",
        "def pick_opponent(policy_pool, manual_strategies):\n",
        "    \"\"\"\n",
        "    Randomly pick an opponent from a combined pool:\n",
        "      - Some fraction from the policy_pool\n",
        "      - Some fraction from the manual_strategies\n",
        "    You can tune the ratio of selection.\n",
        "    \"\"\"\n",
        "    if policy_pool != [] and random.random() < 0.7:\n",
        "        return random.choice(policy_pool)\n",
        "    return random.choice(manual_strategies)\n",
        "\n",
        "def get_action_b(history, opponent, device='cpu'):\n",
        "    \"\"\"\n",
        "    If opponent is a TransformerPolicy, we forward-pass it.\n",
        "    If opponent is a manual strategy function, we just call it.\n",
        "    \"\"\"\n",
        "    # history is a list of (a, b) pairs so far\n",
        "    # Let's gather separate lists of A and B moves for manual strategies\n",
        "    historyA = [ab[0] for ab in history]\n",
        "    historyB = [ab[1] for ab in history]\n",
        "\n",
        "    if isinstance(opponent, nn.Module):\n",
        "        # Opponent is a frozen policy\n",
        "        # We do the same tokenization as for A\n",
        "        pair_indices = [pair_to_int[p] for p in history]\n",
        "        history_tokens = torch.tensor([pair_indices], dtype=torch.long, device=device)\n",
        "        if len(pair_indices) == 0:\n",
        "            # always cooperate on first move\n",
        "            actionB = torch.tensor([0], device=device)  # cooperate\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                logitsB = opponent(history_tokens)\n",
        "                probsB = F.softmax(logitsB, dim=-1)  # shape [1, 2]\n",
        "                distB = torch.distributions.Categorical(probsB)\n",
        "                actionB = distB.sample()  # shape [1]\n",
        "        return actionB.item()\n",
        "    else:\n",
        "        # Opponent is a manual strategy function\n",
        "        return opponent(historyA, historyB)"
      ],
      "metadata": {
        "id": "a9oaT0d0sDaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def play_one_episode(env, policyA, opponent, device='cpu'):\n",
        "    \"\"\"\n",
        "    Play one IPD episode from A's perspective against a chosen 'opponent'.\n",
        "    Returns:\n",
        "      - actionA_logprobs: list of log-probs for A's actions\n",
        "      - rewardsA: list of reward(A) for each step\n",
        "    \"\"\"\n",
        "    history = env.reset()  # empty at start\n",
        "    done = False\n",
        "\n",
        "    actionA_logprobs = []\n",
        "    rewardsA = []\n",
        "\n",
        "    while not done:\n",
        "        # Convert Aâ€™s observation to tokens\n",
        "        pair_indices = [pair_to_int[p] for p in history]  # e.g. [0,1,3,...]\n",
        "        history_tokens = torch.tensor([pair_indices], dtype=torch.long, device=device)\n",
        "\n",
        "        # Forward pass for A\n",
        "        if len(pair_indices) == 0:\n",
        "            # always cooperate on first move\n",
        "            actionA = torch.tensor([0], device=device)  # cooperate\n",
        "            logprobA = torch.tensor([0.0], device=device)  # dummy log-prob\n",
        "        else:\n",
        "            logitsA = policyA(history_tokens)\n",
        "            probsA = F.softmax(logitsA, dim=-1)  # [1, 2]\n",
        "            distA = torch.distributions.Categorical(probsA)\n",
        "            actionA = distA.sample()  # shape [1]\n",
        "            logprobA = distA.log_prob(actionA)\n",
        "\n",
        "        # Opponent's move\n",
        "        actionB = get_action_b(history, opponent, device=device)\n",
        "\n",
        "        # Step environment\n",
        "        history, rA, _, done = env.step(actionA.item(), actionB)\n",
        "\n",
        "        # Store data\n",
        "        actionA_logprobs.append(logprobA)\n",
        "        rewardsA.append(rA)\n",
        "\n",
        "    return actionA_logprobs, rewardsA"
      ],
      "metadata": {
        "id": "m4JPBGWaRJhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_returns(rewards, gamma=1.0):\n",
        "    \"\"\"\n",
        "    Compute discounted returns for REINFORCE.\n",
        "    If gamma=1.0 => no discount, just sum of future rewards.\n",
        "    \"\"\"\n",
        "    returns = []\n",
        "    R = 0\n",
        "    for r in reversed(rewards):\n",
        "        R = r + gamma * R\n",
        "        returns.insert(0, R)\n",
        "    return returns\n",
        "\n",
        "def train_against_mixture(policyA,\n",
        "                          policy_pool,\n",
        "                          manual_strategies,\n",
        "                          episodes=5000,\n",
        "                          batch_size=10,       # Number of episodes per update\n",
        "                          gamma=1.0,\n",
        "                          lr=1e-3,\n",
        "                          max_steps=200,\n",
        "                          snapshot_interval=100,\n",
        "                          device='cpu'):\n",
        "    \"\"\"\n",
        "    Trains A only (purely for A's rewards),\n",
        "    while B is chosen from a mixture of older policies & manual strategies.\n",
        "    \"\"\"\n",
        "    env = IteratedPrisonersDilemma(max_steps=max_steps)\n",
        "    optimizer = optim.Adam(policyA.parameters(), lr=lr)\n",
        "    policyA.to(device)\n",
        "\n",
        "    episode_returns = []\n",
        "\n",
        "    episode_count = 0\n",
        "    while episode_count < episodes:\n",
        "        print(f\"Episode: {episode_count}\")\n",
        "\n",
        "        # Lists to store data across multiple episodes\n",
        "        all_logprobs = []\n",
        "        all_returns = []\n",
        "\n",
        "        # Gather 'batch_size' episodes\n",
        "        for _ in range(batch_size):\n",
        "            # Randomly pick an opponent from the pool for this episode\n",
        "            opponent = pick_opponent(policy_pool, manual_strategies)\n",
        "\n",
        "            # Play one episode\n",
        "            actionA_logprobs, rewardsA = play_one_episode(env, policyA, opponent, device=device)\n",
        "\n",
        "            episode_return_A = sum(rewardsA)\n",
        "            episode_returns.append(episode_return_A)\n",
        "\n",
        "            # Compute returns for A\n",
        "            returnsA = compute_returns(rewardsA, gamma=gamma)\n",
        "\n",
        "            all_logprobs.extend(actionA_logprobs)\n",
        "            all_returns.extend(returnsA)\n",
        "\n",
        "            episode_count += 1\n",
        "            if episode_count >= episodes:\n",
        "                break\n",
        "\n",
        "        # REINFORCE loss for A\n",
        "        logprobs_t = torch.cat(all_logprobs)  # shape [N] where N=number of steps\n",
        "        returns_t = torch.tensor(all_returns, dtype=torch.float32, device=device)\n",
        "\n",
        "        advantages_t = returns_t - returns_t.mean()\n",
        "\n",
        "        loss = -(logprobs_t * advantages_t).mean()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(policyA.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        # Occasionally snapshot A's policy to put into policy_pool\n",
        "        if episode_count % snapshot_interval == 0:\n",
        "            if len(policy_pool) == 10:\n",
        "                policy_pool.pop(0)\n",
        "            policy_pool.append(snapshot_policy(policyA))\n",
        "\n",
        "        if episode_count % 100 == 0:\n",
        "            avg_return = sum(episode_returns[-100:]) / 100  # undiscounted total average\n",
        "            print(f\"Episode {episode_count}: total A reward = {avg_return}, loss={loss.item():.3f}\")\n",
        "\n",
        "    plt.plot(episode_returns)\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Return\")\n",
        "    plt.title(\"Training Progress\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "7KiOsuntRtB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "num_rounds = 200\n",
        "\n",
        "# Create A's policy\n",
        "policyA = TransformerPolicy(max_length=num_rounds, d_model=32, nhead=4, num_layers=2, device=device)\n",
        "\n",
        "# Create an empty policy pool (or maybe with an initial snapshot)\n",
        "policy_pool = []  # later we'll add snapshots of A\n",
        "\n",
        "# Train A\n",
        "train_against_mixture(policyA,\n",
        "                      policy_pool,\n",
        "                      manual_strategies,\n",
        "                      episodes=5000,\n",
        "                      batch_size=10,\n",
        "                      gamma=1.0,\n",
        "                      lr=1e-3,\n",
        "                      max_steps=num_rounds,\n",
        "                      snapshot_interval=100,\n",
        "                      device=device)\n",
        "\n",
        "# After training, you can run play_one_episode(...)\n",
        "# and inspect the learned behavior."
      ],
      "metadata": {
        "id": "zKcQOJSBSv7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EqZngadzBfrB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}